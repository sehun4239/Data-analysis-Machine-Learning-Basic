{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"colab_mnist_cnn_ft1.15.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[],"mount_file_id":"1cb5uQDpOjSendTW7aJ7y-rnddz_0rC6G","authorship_tag":"ABX9TyOnJk4N3JUfwz89OSGHgJ/+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ppOcwIeLT0DR"},"source":["!pip install tensorflow==1.15"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ynDLfpJmS13L"},"source":["import tensorflow as tf\n","tf.__version__"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dd3pnHwxSJ76"},"source":["%reset\n","\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import classification_report\n","\n","# Raw Data Loading\n","df = pd.read_csv('/content/drive/My Drive/[Machine Learning]/data/mnist/train.csv')\n","\n","# 결측치와 이상치는 없어요!!\n","# Data Split(Train data와 Test data 분리)\n","x_data_train, x_data_test, t_data_train, t_data_test = \\\n","train_test_split(df.drop('label', axis=1, inplace=False),\n","                 df['label'],\n","                 test_size=0.3,\n","                 random_state=0)\n","\n","# Min-Max Normalization\n","scaler = MinMaxScaler()\n","scaler.fit(x_data_train)\n","x_data_train_norm = scaler.transform(x_data_train)\n","x_data_test_norm = scaler.transform(x_data_test)\n","\n","del x_data_train, x_data_test\n","\n","### Tensorflow implementation ###\\\n","sess = tf.Session()\n","\n","t_data_train_onehot = sess.run(tf.one_hot(t_data_train, depth=10))\n","t_data_test_onehot = sess.run(tf.one_hot(t_data_test, depth=10))\n","\n","# Placeholder\n","X = tf.placeholder(shape=[None,784], dtype=tf.float32)\n","T = tf.placeholder(shape=[None,10], dtype=tf.float32)\n","drop_rate = tf.placeholder(dtype=tf.float32)\n","\n","# Convolution\n","\n","# 입력데이터 형태부터 설정\n","x_img = tf.reshape(X,[-1, 28, 28, 1])   # (이미지개수, height, width, channel)\n","\n","# convolution layer 1\n","W1 = tf.Variable(tf.random.normal([3,3,1,32]))  # (filter height, \n","                                                #  filter width, \n","                                                #  filter channel, \n","                                                #  filter 개수)\n","L1 = tf.nn.conv2d(x_img,W1, strides=[1,1,1,1], padding='SAME')\n","L1 = tf.nn.relu(L1)   # 이 작업의 결과 => activation map (None,28,28,32)\n","\n","# pooling layer1\n","L1 = tf.nn.max_pool(L1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n","print('L1의 shape : {}'.format(L1.shape))\n","\n","# convolution layer 2\n","W2 = tf.Variable(tf.random.normal([3,3,32,64]))  # (filter height, \n","                                                 #  filter width, \n","                                                 #  filter channel, \n","                                                 #  filter 개수)\n","L2 = tf.nn.conv2d(L1,W2, strides=[1,1,1,1], padding='SAME')\n","L2 = tf.nn.relu(L2)   # 이 작업의 결과 => activation map \n","\n","# pooling layer 2\n","L2 = tf.nn.max_pool(L2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n","print('L2의 shape : {}'.format(L2.shape))  # (?, 7, 7, 64)\n","\n","# FC에 넣어서 학습을 진행해야 해요!\n","# 그래서 FC layer에 넣기 위해 데이터를 Flatten처리(1차원으로 만들어요!)\n","\n","L2 = tf.reshape(L2, [-1,7*7*64])\n","\n","# Weight & bias\n","W3 = tf.get_variable('weight3', shape=[7*7*64,256],\n","                     initializer=tf.contrib.layers.variance_scaling_initializer())\n","b3 = tf.Variable(tf.random.normal([256]))\n","\n","_layer3 = tf.nn.relu(tf.matmul(L2,W3) + b3)\n","layer3 = tf.nn.dropout(_layer3, rate=drop_rate)\n","\n","W4 = tf.get_variable('weight4', shape=[256,10],\n","                     initializer=tf.contrib.layers.variance_scaling_initializer())\n","b4 = tf.Variable(tf.random.normal([10]))\n","\n","# Hypothesis\n","logit = tf.matmul(layer3,W4) + b4\n","H = tf.nn.softmax(logit)\n","\n","# loss\n","loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit,\n","                                                                 labels=T))\n","\n","# train\n","train = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(loss)\n","\n","# parameter\n","num_of_epoch = 200\n","batch_size = 100\n","\n","# 학습\n","def run_train(sess, train_x, train_t):\n","    print('### Starting Training ###')\n","    # 초기화\n","    sess.run(tf.global_variables_initializer())\n","    \n","    for step in range(num_of_epoch):\n","        total_batch = int(train_x.shape[0] / batch_size)\n","\n","        for i in range(total_batch):\n","            batch_x = train_x[i*batch_size:(i+1)*batch_size]\n","            batch_t = train_t[i*batch_size:(i+1)*batch_size]\n","            _, loss_val = sess.run([train,loss], feed_dict={X:batch_x, \n","                                                            T:batch_t, \n","                                                            drop_rate:0.4})\n","\n","        if step % 20 == 0:\n","            print('Loss : {}'.format(loss_val))\n","    print('### End Training ###')\n","\n","    \n","# Accuracy\n","predict = tf.argmax(H,1)\n","\n","# sklearn의 classification_report를 이용한 성능평가\n","run_train(sess,x_data_train_norm,t_data_train_onehot)\n","target_names=['num 0','num 1','num 2','num 3','num 4','num 5','num 6','num 7','num 8','num 9']\n","print(classification_report(t_data_test,\n","                            sess.run(predict,\n","                                     feed_dict={X:x_data_test_norm, \n","                                                drop_rate:0 }),\n","                            target_names=target_names))                                                "],"execution_count":null,"outputs":[]}]}