{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Starting Training ###\n",
      "Loss : 1.5366599559783936\n",
      "Loss : 0.5853803753852844\n",
      "### End Training ###\n",
      "### Test Set으로 Accuracy 측정 ###\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.87      0.84       168\n",
      "           1       0.76      0.68      0.72       100\n",
      "\n",
      "    accuracy                           0.80       268\n",
      "   macro avg       0.79      0.77      0.78       268\n",
      "weighted avg       0.80      0.80      0.80       268\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Raw Data Loding\n",
    "train = pd.read_csv('./data/titanic/train.csv')\n",
    "train2 = pd.read_csv('./data/titanic/train.csv')\n",
    "test = pd.read_csv('./data/titanic/test.csv')\n",
    "\n",
    "# 필요없는 column\n",
    "train.drop(['Survived', 'PassengerId', 'Name', 'Ticket', 'Fare', 'Cabin'], axis=1, inplace=True)\n",
    "test.drop(['PassengerId', 'Name', 'Ticket', 'Fare', 'Cabin'], axis=1, inplace=True)\n",
    "\n",
    "# 성별처리\n",
    "sex_mapping = { 'male' : 0, 'female' : 1 }\n",
    "train['Sex'] = train['Sex'].map(sex_mapping)\n",
    "test['Sex'] = test['Sex'].map(sex_mapping)\n",
    "\n",
    "# 가족처리\n",
    "train['Family'] = train['SibSp'] + train['Parch']\n",
    "train.drop(['SibSp', 'Parch'], axis=1, inplace=True)\n",
    "\n",
    "test['Family'] = test['SibSp'] + test['Parch']\n",
    "test.drop(['SibSp', 'Parch'], axis=1, inplace=True)\n",
    "\n",
    "# Embarked 결측치 처리\n",
    "train['Embarked'] = train['Embarked'].fillna('Q')\n",
    "test['Embarked'] = test['Embarked'].fillna('Q')\n",
    "\n",
    "# Age에 대한 결측치 처리\n",
    "train['Age'] = train['Age'].fillna(train['Age'].mean())\n",
    "test['Age'] = test['Age'].fillna(test['Age'].mean())\n",
    "\n",
    "# Embarked 문자 -> 숫자 처리\n",
    "embarked_mapping = {'S' : 0 , 'C': 1, 'Q': 2 }\n",
    "train['Embarked'] = train['Embarked'].map(embarked_mapping)\n",
    "test['Embarked'] = test['Embarked'].map(embarked_mapping)\n",
    "\n",
    "# Age에 대해서 Binning 처리 (Numerical value -> categorical value)\n",
    "train.loc[train['Age'] < 8, 'Age'] = 0\n",
    "train.loc[(train['Age'] >= 8) & (train['Age'] < 20), 'Age'] = 1\n",
    "train.loc[(train['Age'] >=20) & (train['Age'] < 65 ), 'Age'] = 2\n",
    "train.loc[train['Age'] >=65, 'Age'] = 3\n",
    "\n",
    "test.loc[test['Age'] < 8, 'Age'] = 0\n",
    "test.loc[(test['Age'] >= 8) & (test['Age'] < 20), 'Age'] = 1\n",
    "test.loc[(test['Age'] >=20) & (test['Age'] < 65 ), 'Age'] = 2\n",
    "test.loc[test['Age'] >=65, 'Age'] = 3\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Data Split\n",
    "x_data_train, x_data_test, t_data_train, t_data_test = \\\n",
    "train_test_split(train, train2['Survived'], test_size=0.3, random_state=0)\n",
    "\n",
    "# Min-Max Normalization\n",
    "scaler = MinMaxScaler()   # scaler = StandardScaler()\n",
    "scaler.fit(x_data_train)\n",
    "x_data_train_norm = scaler.transform(x_data_train)\n",
    "x_data_test_norm = scaler.transform(x_data_test)\n",
    "\n",
    "del x_data_train\n",
    "del x_data_test\n",
    "\n",
    "#Tensorflow\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "t_data_train_onehot = sess.run(tf.one_hot(t_data_train,depth=2))\n",
    "t_data_test_onehot = sess.run(tf.one_hot(t_data_test,depth=2))\n",
    "\n",
    "# Placeholder\n",
    "X = tf.placeholder(shape=[None,5], dtype=tf.float32)\n",
    "T = tf.placeholder(shape=[None,2], dtype=tf.float32)\n",
    "drop_rate = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "W2 = tf.get_variable('weight2', shape=[5,4],\n",
    "                     initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "\n",
    "b2 = tf.Variable(tf.random.normal([4]), name='bias2')\n",
    "_layer2 = tf.nn.relu(tf.matmul(X,W2) + b2)\n",
    "layer2 = tf.nn.dropout(_layer2, rate=drop_rate)\n",
    "\n",
    "W3 = tf.get_variable('weight3', shape=[4,3],\n",
    "                     initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "\n",
    "b3 = tf.Variable(tf.random.normal([3]), name='bias3')\n",
    "_layer3 = tf.nn.relu(tf.matmul(layer2,W3) + b3)\n",
    "layer3 = tf.nn.dropout(_layer3, rate=drop_rate)\n",
    "\n",
    "W4 = tf.get_variable('weight4', shape=[3,2],\n",
    "                     initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "b4 = tf.Variable(tf.random.normal([2]), name='bias4')\n",
    "\n",
    "# Hypothesis\n",
    "logit = tf.matmul(layer3,W4) + b4\n",
    "H = tf.nn.softmax(logit)\n",
    "\n",
    "# loss\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, \n",
    "                                                                 labels=T))\n",
    "\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-2).minimize(loss)\n",
    "\n",
    "# parameter\n",
    "num_of_epoch = 2000\n",
    "batch_size = 100\n",
    "\n",
    "# 학습\n",
    "def run_train(sess, train_x, train_t):\n",
    "    print('### Starting Training ###')\n",
    "    # 초기화\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(num_of_epoch):\n",
    "        total_batch = int(train_x.shape[0] / batch_size)\n",
    "\n",
    "        for i in range(total_batch):\n",
    "            batch_x = train_x[i*batch_size:(i+1)*batch_size]\n",
    "            batch_t = train_t[i*batch_size:(i+1)*batch_size]\n",
    "            _, loss_val = sess.run([train,loss], feed_dict={X:batch_x, \n",
    "                                                            T:batch_t,\n",
    "                                                            drop_rate:0.3})\n",
    "\n",
    "        if step % 1000 == 0:\n",
    "            print('Loss : {}'.format(loss_val))\n",
    "    print('### End Training ###')\n",
    "\n",
    "    \n",
    "# Accuracy\n",
    "predict = tf.argmax(H,1)\n",
    "correct = tf.equal(predict, tf.argmax(T,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "\n",
    "# Testing\n",
    "run_train(sess,x_data_train_norm,t_data_train_onehot) # 학습\n",
    "print('### Test Set으로 Accuracy 측정 ###')\n",
    "result = sess.run(predict, feed_dict={X:x_data_test_norm, \n",
    "                                      drop_rate:0})\n",
    "print(classification_report(t_data_test,result.ravel()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0\n",
      "1      1\n",
      "2      0\n",
      "3      0\n",
      "4      1\n",
      "      ..\n",
      "413    0\n",
      "414    1\n",
      "415    0\n",
      "416    0\n",
      "417    0\n",
      "Name: Survived, Length: 418, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "test_scaled = scaler.transform(test)\n",
    "\n",
    "result = sess.run(H, feed_dict = {X:test_scaled,\n",
    "                                  drop_rate:0})\n",
    "result = np.argmax(result, axis=1)\n",
    "result = pd.Series(result, name='Survived')\n",
    "\n",
    "print(result)\n",
    "\n",
    "submission = pd.read_csv('./data/titanic/test.csv')\n",
    "submission['Survived'] = result\n",
    "submission = submission[['PassengerId', 'Survived']]\n",
    "\n",
    "submission.to_csv('./submission/titanic_deep4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### cross validation ###\n",
      "score : [0.98       0.98642857 0.985      0.97642857 0.98642857 0.98428571\n",
      " 0.98714286 0.97714286 0.97714286 0.98642857]\n",
      "평균: 0.9826428571428572\n",
      "우리 Model의 최종 Accuracy : 0.9845\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# Multinomial Classification\n",
    "# BMI 지수로 학습해보자. => 키와 몸무게를 가지고 저체중, 정상 과체중 비만을 판단하는 지수\n",
    "# BMI = 자신의 몸무게(kg) / 키의 제곱(m)\n",
    "#      18.5 이하 => 저체중\n",
    "#      18.5 ~ 23 => 정상\n",
    "#      23 ~ 25 => 과체중\n",
    "#      25 ~ => 비만\n",
    "# 우리가 하려는 건 식이 아니라 BMI 지수를 조사한 데이터가 있다.\n",
    "# 이걸 학습해서 예측을 통해 나의 BMI 지수를 알아보자\n",
    "# 단 제공하는 데이터는 4가지가 아니라 3가지 분류로 되어있다.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "df = pd.read_csv('./data/bmi/bmi.csv', skiprows=3)\n",
    "\n",
    "# display(df)\n",
    "\n",
    "# 결측치 확인\n",
    "# df.isnull().sum()        # 결측치 없음\n",
    "\n",
    "# 이상치 확인\n",
    "zscore = 1.8\n",
    "\n",
    "# 이상치를 확인\n",
    "# df.loc[np.abs(stats.zscore(df['height'])) >= zscore, :] # height의 이상치는 없다\n",
    "# df.loc[np.abs(stats.zscore(df['weight'])) >= zscore, :] # weight의 이상치는 없다\n",
    "# df.loc[np.abs(stats.zscore(df['label'])) >= zscore, :] # label의 이상치는 없다\n",
    "\n",
    "# Data Split\n",
    "# Train, Test 두 부분으로 분할. 분리하는 비율은 7:3으로 분리\n",
    "# 나중에 Train부분은 k-fold cross validation을 진행\n",
    "x_data_train, x_data_test, t_data_train, t_data_test = \\\n",
    "train_test_split(df[['height','weight']],df['label'],test_size = 0.3, random_state=0) # 14000 / 6000\n",
    "\n",
    "# Normalization\n",
    "scaler = MinMaxScaler()  # scaler 객체를 생성\n",
    "scaler.fit(x_data_train) # scaler 객체에 최대 최소와 같은 정보가 들어간다. (fit 처리)\n",
    "\n",
    "x_data_train_norm = scaler.transform(x_data_train)\n",
    "x_data_test_norm = scaler.transform(x_data_test)\n",
    "\n",
    "del x_data_train       # 혼동 방지를 위해 변수를 삭제\n",
    "del x_data_test\n",
    "\n",
    "# sklearn 구현은 매우매우 간단 - model 생성하고 학습진행\n",
    "model = LogisticRegression()\n",
    "model.fit(x_data_train_norm, t_data_train)\n",
    "\n",
    "# 우리 model의 정확도를 측정해야한다.\n",
    "# cross validation\n",
    "kfold = 10\n",
    "kfold_score = cross_val_score(model,x_data_train_norm, t_data_train, cv=kfold)\n",
    "print('### cross validation ###')\n",
    "print('score : {}'.format(kfold_score))\n",
    "print('평균: {}'.format(kfold_score.mean()))\n",
    "\n",
    "# 최종모델평가\n",
    "predict_val = model.predict(x_data_test_norm)  # 테스트 데이터로 예측값을 구해요\n",
    "acc = accuracy_score(predict_val, t_data_test)\n",
    "\n",
    "print('우리 Model의 최종 Accuracy : {}'.format(acc))\n",
    "\n",
    "# Predict\n",
    "\n",
    "height = 188\n",
    "weight = 78\n",
    "my_state = [[height,weight]]\n",
    "my_state_val = model.predict(scaler.transform(my_state))\n",
    "print(my_state_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data_env] *",
   "language": "python",
   "name": "conda-env-data_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
