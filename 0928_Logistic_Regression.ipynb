{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9월 28일 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## python 결과값 ##########\n",
      "공부시간 : [[13]], 결과 : (0, array([[0.39999997]]))\n"
     ]
    }
   ],
   "source": [
    "# %reset          # 메모리 초기화\n",
    "\n",
    "# Logistic Regression을 python, tensorflow, sklearn으로 각각 구현\n",
    "# 처음은 독립변수가 1개인 걸로 하자\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import linear_model\n",
    "\n",
    "# 수치미분함수 들고와서 쓰자\n",
    "###########################\n",
    "def numerical_derivative(f,x):\n",
    "    # f : 미분하려고 하는 다변수 함수\n",
    "    # x : 모든 변수를 포함하고 있는 ndarray (차원 상관없이)\n",
    "    \n",
    "    delta_x = 1e-4\n",
    "    derivative_x = np.zeros_like(x)  # 미분한 결과를 저장하는 ndarray\n",
    "    \n",
    "    # iterator를 이용해서 입력변수 x에 대해 편미분을 수행\n",
    "    it = np.nditer(x, flags=['multi_index'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        idx = it.multi_index    # iterator의 현재 index를 추출 (tuple)\n",
    "        \n",
    "        # 현재 칸의 값을 어딘가에 잠시 저장해야한다.\n",
    "        tmp = x[idx]\n",
    "        \n",
    "        x[idx] = tmp + delta_x\n",
    "        fx_plus_delta = f(x)   # f(x + delta_x)\n",
    "        \n",
    "        x[idx] = tmp - delta_x\n",
    "        fx_minus_delta = f(x)   # f(x - delta_x)\n",
    "        \n",
    "        derivative_x[idx] = (fx_plus_delta - fx_minus_delta) / (2 * delta_x)\n",
    "        \n",
    "        x[idx] = tmp     # 데이터를 원상 복구\n",
    "        \n",
    "        it.iternext()\n",
    "        \n",
    "    return derivative_x\n",
    "\n",
    "##########################\n",
    "# Raw Data Loading + Data Preprocessing\n",
    "# 이번 예제에는 이 과정 필요 없음\n",
    "\n",
    "# Training Data Set\n",
    "# 지도학습을 하고 있기 때문에 독립변수와 종속변수 (label)로 구분해서 데이터 준비\n",
    "# 어떤 경우에는 이 두객를 아예 분리해서 제공하는 경우도 있다. 참고\n",
    "\n",
    "x_data = np.arange(2,21,2).reshape(-1,1)\n",
    "t_data = np.array([0,0,0,0,0,0,1,1,1,1])\n",
    "\n",
    "###########################\n",
    "# python 구현부터 해보자\n",
    "\n",
    "# Weight & bias\n",
    "W = np.random.rand(1,1)\n",
    "b = np.random.rand(1)\n",
    "\n",
    "# 위에서 정의한 W와 b의 값을 구해야 한다.\n",
    "# 이 값만 구하면 우리의 최종 목적인 model을 완성할 수 있다.\n",
    "\n",
    "# loss function(손실함수) - 우리 모델의 예측값과 들어온 t_data(정답)\n",
    "# 입력으로 들어온 x_data와 W,b 값을 이용해 예측값 계산, t_data(정답)을 비교함\n",
    "def loss_func(input_obj):     # input_obj : W와 b를 같이 포함하는 ndarray [W1 W2 b]\n",
    "    \n",
    "    num_of_bias = b.shape[0]   # num_of_bias : 1\n",
    "    input_W = input_obj[:-1*num_of_bias].reshape(-1,num_of_bias)  # W 생성\n",
    "    input_b = input_obj[-1*num_of_bias]\n",
    "    \n",
    "    # 우리 모델의 예측값은 : (linear regression model(Wx+b) => sigmoid를 적용)\n",
    "    z = np.dot(x_data, input_W) + input_b\n",
    "    y = 1 / ( 1 + np.exp(-1 * z) )\n",
    "    \n",
    "    delta = 1e-7 # 굉장히 작은값을 이용해서 프로그램으로 로그연산시 \n",
    "                 # 무한대로 발산하는 것 방지\n",
    "    \n",
    "    # cross entropy\n",
    "    return -np.sum(t_data*np.log(y+delta) + ((1-t_data)*np.log(1-y+delta)))\n",
    "\n",
    "# learning rate\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# 학습\n",
    "for step in range(30000):\n",
    "    input_param = np.concatenate((W.ravel(), b.ravel()), axis=0)  # [W1 W2 b]\n",
    "    derivative_result = learning_rate * numerical_derivative(loss_func, input_param)\n",
    "    \n",
    "    num_of_bias = b.shape[0]\n",
    "    \n",
    "    W = W - derivative_result[:-1*num_of_bias].reshape(-1,num_of_bias) # [[W1] [W2]]\n",
    "    b = b - derivative_result[-1*num_of_bias:]\n",
    "    \n",
    "# predict => W,b를 다 구해서 우리의 logistic Regression model을 완성\n",
    "def logistic_predict(x):   # 공부한 시간이 입력\n",
    "    z = np.dot(x,W) + b\n",
    "    y = 1 / ( 1 + np.exp(-1*z) )\n",
    "    \n",
    "    if y < 0.5 :\n",
    "        result = 0\n",
    "    else:\n",
    "        result = 1\n",
    "        \n",
    "    return result, y\n",
    "\n",
    "study_hour = np.array([[13]])\n",
    "result = logistic_predict(study_hour)\n",
    "print('######## python 결과값 ##########')\n",
    "print('공부시간 : {}, 결과 : {}'.format(study_hour,result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## sklearn 결과값 ##########\n",
      "공부시간 : [[13]], 결과 : [0],[[0.50009391 0.49990609]]\n"
     ]
    }
   ],
   "source": [
    "### sklearn으로 구현해보자\n",
    "\n",
    "# logistic regression model 생성\n",
    "model = linear_model.LogisticRegression()\n",
    "\n",
    "# training data set을 이용해서 학습\n",
    "model.fit(x_data,t_data.ravel())       # x는 2차원, label은 1차원 vector로 넣어야함\n",
    "\n",
    "study_hour = np.array([[13]])\n",
    "predict_val = model.predict(study_hour)\n",
    "predict_prob = model.predict_proba(study_hour)\n",
    "\n",
    "print('######## sklearn 결과값 ##########')\n",
    "print('공부시간 : {}, 결과 : {},{}'.format(study_hour,predict_val,predict_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## tensorflow 결과값 ##########\n",
      "공부시간 : [13], 결과 : [0.40000007]\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(dtype=tf.float32) # 독립변수가 1개인 경우 shape을 명시하지 않음\n",
    "T = tf.placeholder(dtype=tf.float32) # t_data\n",
    "\n",
    "# Weight & bias\n",
    "W = tf.Variable(tf.random.normal([1,1]), name='weight')\n",
    "W = tf.Variable(tf.random.normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis\n",
    "logit = W * X + b  # matrix 곱연산 하지 않나요 ??\n",
    "H = tf.sigmoid(logit)\n",
    "\n",
    "# loss function\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit, labels=T))\n",
    "\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-3).minimize(loss)\n",
    "\n",
    "# session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "for step in range(30000):\n",
    "    sess.run(train, feed_dict={X:x_data, T:t_data})\n",
    "    \n",
    "study_hour = np.array([13])\n",
    "result = sess.run(H,feed_dict={X:study_hour})\n",
    "print('######## tensorflow 결과값 ##########')\n",
    "print('공부시간 : {}, 결과 : {}'.format(study_hour,result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "admit    0\n",
      "gre      0\n",
      "gpa      0\n",
      "rank     0\n",
      "dtype: int64\n",
      "(382, 4)\n",
      "###### sklearn 결과 ######\n",
      "[[600.    3.8   1. ]] [1] [[0.43740782 0.56259218]]\n",
      "W:[[ 0.70904505]\n",
      " [-0.14659591]\n",
      " [-0.09499577]],b:[-0.53022546],loss:0.6417478919029236\n",
      "W:[[ 0.6460832 ]\n",
      " [-0.1878663 ]\n",
      " [-0.27333695]],b:[-0.71348506],loss:0.6152343153953552\n",
      "W:[[ 0.65193063]\n",
      " [-0.15737596]\n",
      " [-0.38120386]],b:[-0.7708575],loss:0.6097505688667297\n",
      "W:[[ 0.6744258 ]\n",
      " [-0.11012489]\n",
      " [-0.4665115 ]],b:[-0.7947191],loss:0.6062067151069641\n",
      "W:[[ 0.7000656 ]\n",
      " [-0.06026416]\n",
      " [-0.5418601 ]],b:[-0.80907273],loss:0.6031953692436218\n",
      "W:[[ 0.72520995]\n",
      " [-0.01160642]\n",
      " [-0.6109964 ]],b:[-0.8205985],loss:0.6005567312240601\n",
      "W:[[ 0.7489259 ]\n",
      " [ 0.03488902]\n",
      " [-0.67522347]],b:[-0.83119816],loss:0.5982351899147034\n",
      "W:[[ 0.7710455 ]\n",
      " [ 0.07906872]\n",
      " [-0.735134  ]],b:[-0.84141755],loss:0.5961896777153015\n",
      "W:[[ 0.79161364]\n",
      " [ 0.1210006 ]\n",
      " [-0.79110485]],b:[-0.8513822],loss:0.5943844318389893\n",
      "W:[[ 0.810732  ]\n",
      " [ 0.16080914]\n",
      " [-0.8434311 ]],b:[-0.861151],loss:0.5927892327308655\n",
      "######## tensorflow 결과값 ##########\n",
      "점수 : [[600.    3.8   1. ]], 결과 : [[0.43791625]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Multi Variable Logistic Regression\n",
    "# 독립변수가 2개 이상인 logistic regression\n",
    "\n",
    "# 학습하는 데이터는 GRE(Graduate Record Examination)와\n",
    "# GPA 성적 그리고 Rank에 대한 대학원 합격/불합격 정보\n",
    "\n",
    "# 내 성적 [600. 3.8 1.] 의 결과\n",
    "# 첫번째는 sklearn으로 구현하기\n",
    "\n",
    "# Raw Data Loading \n",
    "df = pd.read_csv('./data/admission/admission.csv')\n",
    "\n",
    "# 결측치 확인\n",
    "print(df.isnull().sum())     # 결측치가 없음 -> dropna 안해도됨\n",
    "\n",
    "# 이상치를 확인해서 있으면 제거\n",
    "# fig = plt.figure()\n",
    "# fig_admit = fig.add_subplot(1,4,1)\n",
    "# fig_gre = fig.add_subplot(1,4,2)\n",
    "# fig_gpa = fig.add_subplot(1,4,3)\n",
    "# fig_rank = fig.add_subplot(1,4,4)\n",
    "\n",
    "# fig_admit.boxplot(df['admit'])\n",
    "# fig_gre.boxplot(df['gre'])\n",
    "# fig_gpa.boxplot(df['gpa'])\n",
    "# fig_rank.boxplot(df['rank'])\n",
    "\n",
    "# fig.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# 확인했더니 이상치가 있어서 제거하려한다.\n",
    "zscore_threshold = 2.0\n",
    "\n",
    "for col in df.columns:\n",
    "    outlier = df[col][np.abs(stats.zscore(df[col])) > zscore_threshold]\n",
    "    df = df[~df[col].isin(outlier)]\n",
    "    \n",
    "print(df.shape)        # (382, 4)\n",
    "\n",
    "# Training Data Set\n",
    "x_data = df.drop('admit', axis=1, inplace=False).values\n",
    "t_data = df['admit'].values.reshape(-1,1)\n",
    "\n",
    "\n",
    "# 정규화를 진행해야한다.\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_x.fit(x_data)\n",
    "norm_x_data = scaler_x.transform(x_data)   # for python, tensorflow\n",
    "\n",
    "# sklearn을 이용한 구현\n",
    "model = linear_model.LogisticRegression()\n",
    "model.fit(x_data,t_data.ravel())\n",
    "print('###### sklearn 결과 ######')\n",
    "my_score = np.array([[600, 3.8, 1]])\n",
    "predict_val = model.predict(my_score)  # 0 or 1\n",
    "predict_proba = model.predict_proba(my_score)  # 불합격/합격 확률\n",
    "print(my_score, predict_val, predict_proba)\n",
    "\n",
    "\n",
    "# Tensorflow\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None, 3], dtype=tf.float32)  # 독립변수의 데이터를 받기위한 placeholder\n",
    "T = tf.placeholder(shape=[None, 1], dtype=tf.float32)  # 종속변수(label)의 placeholder\n",
    "\n",
    "# Weight & bias\n",
    "W = tf.Variable(tf.random.normal([3,1]), name='weight')\n",
    "b = tf.Variable(tf.random.normal([1]), name='bias')\n",
    "\n",
    "# hypothesis\n",
    "logit = tf.matmul(X,W) + b\n",
    "H = tf.sigmoid(logit)\n",
    "\n",
    "# loss function\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit, labels=T))\n",
    "\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-3).minimize(loss)\n",
    "\n",
    "# session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "for step in range(30000):\n",
    "    _, W_val, b_val, loss_val = sess.run([train,W,b,loss], \n",
    "                                         feed_dict={X:norm_x_data, T:t_data})\n",
    "    \n",
    "    if step % 3000 == 0:\n",
    "        print('W:{},b:{},loss:{}'.format(W_val,b_val,loss_val))\n",
    "        \n",
    "my_score = np.array([[600,3.8,1]])\n",
    "scaled_my_score = scaler_x.transform(my_score)\n",
    "\n",
    "result = sess.run(H,feed_dict={X:scaled_my_score})\n",
    "print('######## tensorflow 결과값 ##########')\n",
    "print('점수 : {}, 결과 : {}'.format(my_score,result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "admit    0\n",
      "gre      0\n",
      "gpa      0\n",
      "rank     0\n",
      "dtype: int64\n",
      "(382, 4)\n",
      "W:[[ 1.7646356 ]\n",
      " [ 0.08762144]\n",
      " [-0.2812959 ]],b:[-0.68270737],loss:0.7055816650390625\n",
      "W:[[ 1.5603307 ]\n",
      " [-0.07895922]\n",
      " [-0.5250376 ]],b:[-1.0438235],loss:0.6067104339599609\n",
      "W:[[ 1.5061036 ]\n",
      " [-0.08941314]\n",
      " [-0.6315138 ]],b:[-1.1422116],loss:0.5978860855102539\n",
      "W:[[ 1.4913461 ]\n",
      " [-0.06034793]\n",
      " [-0.7001493 ]],b:[-1.1717452],loss:0.595608651638031\n",
      "W:[[ 1.4872788 ]\n",
      " [-0.02153197]\n",
      " [-0.7557979 ]],b:[-1.1817459],loss:0.5940296053886414\n",
      "W:[[ 1.4858648 ]\n",
      " [ 0.01877024]\n",
      " [-0.8054961 ]],b:[-1.1860178],loss:0.5926572680473328\n",
      "W:[[ 1.4847919 ]\n",
      " [ 0.05822078]\n",
      " [-0.8514294 ]],b:[-1.1885669],loss:0.5914321541786194\n",
      "W:[[ 1.4833894 ]\n",
      " [ 0.09620302]\n",
      " [-0.89437455]],b:[-1.1906039],loss:0.5903341770172119\n",
      "W:[[ 1.4814932 ]\n",
      " [ 0.13258183]\n",
      " [-0.9346981 ]],b:[-1.192392],loss:0.5893484950065613\n",
      "W:[[ 1.4791623 ]\n",
      " [ 0.16740315]\n",
      " [-0.97260696]],b:[-1.1941801],loss:0.5884621739387512\n",
      "0.68586385\n",
      "######## tensorflow 결과값 ##########\n",
      "점수 : [[600.    3.8   1. ]], 결과 : [[0.44530797]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Multi Variable Logistic Regression\n",
    "# 독립변수가 2개 이상인 logistic regression\n",
    "\n",
    "# 학습하는 데이터는 GRE(Graduate Record Examination)와\n",
    "# GPA 성적 그리고 Rank에 대한 대학원 합격/불합격 정보\n",
    "\n",
    "# 내 성적 [600. 3.8 1.] 의 결과\n",
    "# 첫번째는 sklearn으로 구현하기\n",
    "\n",
    "# Raw Data Loading \n",
    "df = pd.read_csv('./data/admission/admission.csv')\n",
    "\n",
    "# 결측치 확인\n",
    "print(df.isnull().sum())     # 결측치가 없음 -> dropna 안해도됨\n",
    "\n",
    "# 이상치를 확인해서 있으면 제거\n",
    "# fig = plt.figure()\n",
    "# fig_admit = fig.add_subplot(1,4,1)\n",
    "# fig_gre = fig.add_subplot(1,4,2)\n",
    "# fig_gpa = fig.add_subplot(1,4,3)\n",
    "# fig_rank = fig.add_subplot(1,4,4)\n",
    "\n",
    "# fig_admit.boxplot(df['admit'])\n",
    "# fig_gre.boxplot(df['gre'])\n",
    "# fig_gpa.boxplot(df['gpa'])\n",
    "# fig_rank.boxplot(df['rank'])\n",
    "\n",
    "# fig.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# 확인했더니 이상치가 있어서 제거하려한다.\n",
    "zscore_threshold = 2.0\n",
    "\n",
    "for col in df.columns:\n",
    "    outlier = df[col][np.abs(stats.zscore(df[col])) > zscore_threshold]\n",
    "    df = df[~df[col].isin(outlier)]\n",
    "    \n",
    "print(df.shape)        # (382, 4)\n",
    "\n",
    "# Training Data Set\n",
    "x_data = df.drop('admit', axis=1, inplace=False).values\n",
    "t_data = df['admit'].values.reshape(-1,1)\n",
    "\n",
    "\n",
    "# 정규화를 진행해야한다.\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_x.fit(x_data)\n",
    "norm_x_data = scaler_x.transform(x_data)   # for python, tensorflow\n",
    "\n",
    "# Tensorflow\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None, 3], dtype=tf.float32)  # 독립변수의 데이터를 받기위한 placeholder\n",
    "T = tf.placeholder(shape=[None, 1], dtype=tf.float32)  # 종속변수(label)의 placeholder\n",
    "\n",
    "# Weight & bias\n",
    "W = tf.Variable(tf.random.normal([3,1]), name='weight')\n",
    "b = tf.Variable(tf.random.normal([1]), name='bias')\n",
    "\n",
    "# hypothesis\n",
    "logit = tf.matmul(X,W) + b\n",
    "H = tf.sigmoid(logit)\n",
    "\n",
    "# loss function\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit, labels=T))\n",
    "\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-3).minimize(loss)\n",
    "\n",
    "# session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "for step in range(30000):\n",
    "    _, W_val, b_val, loss_val = sess.run([train,W,b,loss], \n",
    "                                         feed_dict={X:norm_x_data, T:t_data})\n",
    "    \n",
    "    if step % 3000 == 0:\n",
    "        print('W:{},b:{},loss:{}'.format(W_val,b_val,loss_val))\n",
    "\n",
    "# 정확도(Accuracy) 측정\n",
    "predict = tf.cast(H >= 0.5, dtype=tf.float32) # True =>1, False => 0\n",
    "correct = tf.equal(predict, T)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "\n",
    "accuracy_val = sess.run(accuracy, feed_dict={X:norm_x_data, T:t_data})\n",
    "print(accuracy_val)\n",
    "        \n",
    "my_score = np.array([[600,3.8,1]])\n",
    "scaled_my_score = scaler_x.transform(my_score)\n",
    "\n",
    "result = sess.run(H,feed_dict={X:scaled_my_score})\n",
    "print('######## tensorflow 결과값 ##########')\n",
    "print('점수 : {}, 결과 : {}'.format(my_score,result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data_env] *",
   "language": "python",
   "name": "conda-env-data_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
