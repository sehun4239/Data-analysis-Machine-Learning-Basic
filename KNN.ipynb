{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.55630135 0.18739828]\n",
      " [0.52411839 0.80281788]]\n",
      "tf.Tensor([1.9922442], shape=(1,), dtype=float32)\n",
      "[1.9922442]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# 랜덤값을 하나 얻어온다\n",
    "random1 = np.random.rand(2,2)\n",
    "print(random1)\n",
    "\n",
    "# Tensorflow\n",
    "random2 = tf.random.normal([1], dtype=tf.float32)\n",
    "print(random2)  # Tensor가 출력\n",
    "# TF 1.x버전에서는 node를 실행시키려면 session이 있어야했다.\n",
    "# 하지만 2.x버전부터는 session 없이 즉시 실행 가능하다. (Eager Execution)\n",
    "\n",
    "print(random2.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c의 값은 : 30.0\n",
      "60.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "a = tf.constant(10, dtype=tf.float32)\n",
    "b = tf.constant(20, dtype=tf.float32)\n",
    "\n",
    "c = a + b\n",
    "\n",
    "print('c의 값은 : {}'.format(c.numpy()))\n",
    "\n",
    "d= 30.0\n",
    "\n",
    "tensor_d = tf.convert_to_tensor(d)\n",
    "\n",
    "print((c + tensor_d).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "W = tf.Variable(tf.random.normal([1]), name='weight')\n",
    "\n",
    "# 기존에는 tf.variable()을 이용해서 변수를 만들면 사용하기 전에 반드시 초기화를 진행해야 했다.\n",
    "# TF 2.0부터는 초기화를 안해도 된다.\n",
    "print(W.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow graph에 입력을 주는 부분이 없어졌다.\n",
    "# 기존에는 graph에 데이터를 밀어넣기 위해 placeholder를 이용했다.\n",
    "# Eager Execution에 의해서 이제는 placeholder가 필요 없게 됐다.\n",
    "# placeholder는 삭제되었다.\n",
    "\n",
    "# Lazy execution은 더이상 x eager execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow의 keras를 이용하여 model을 생성해 보자\n",
    "\n",
    "import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "\n",
    "# model = Sequential()  # keras model 생성\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# model을 만들었으니 그 다음에는 layer를 만들어야 한다.\n",
    "model.add(tf.keras.layers.Flatten(input_shape=(2,)))  # layer를 추가\n",
    "model.add(tf.keras.layers.Dense(3, activation='softmax'))\n",
    "\n",
    "# model compile 과정\n",
    "model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "def my_loss:\n",
    "    pass\n",
    "\n",
    "# model 학습\n",
    "model.fit(x_data_train, t_data_train, epochs=100, batch_size=100, validation_split=0.3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(153, 6)\n",
      "Ozone      37\n",
      "Solar.R     7\n",
      "Wind        0\n",
      "Temp        0\n",
      "Month       0\n",
      "Day         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## tensorflow 2.1을 이용하여 Ozone 예제를 다시 구현해보자\n",
    "## Multiple Linear Regression\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential  # Model\n",
    "from tensorflow.keras.layers import Flatten, Dense  # Layer\n",
    "from tensorflow.keras.optimizers import SGD  # Optimizer\n",
    "from sklearn.preprocessing import MinMaxScaler  # Normmalization\n",
    "from scipy import stats   # 이상치 처리\n",
    "\n",
    "# Raw Data Lodaing\n",
    "df = pd.read_csv('./data/ozone.csv')\n",
    "\n",
    "# 결측치 확인 및 처리\n",
    "# 데이터가 충분히 많고 결측치가 적으면 삭제가 답이다. 하지만 일반적으로 결측치를 삭제하면 데이터가 유실\n",
    "print(df.shape)\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9845\n",
      "0.998\n"
     ]
    }
   ],
   "source": [
    "# 우리 KNN의 사용법에 대해서 알아보자\n",
    "# sklearn을 이용해서 알아보자\n",
    "\n",
    "# BMI 예제를 이용해서 학습한 후 정확도를 측정해보자\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Raw Data Loading\n",
    "df = pd.read_csv('./data/bmi/bmi.csv', skiprows=3)\n",
    "\n",
    "# data split\n",
    "x_data_train, x_data_test, t_data_train, t_data_test = \\\n",
    "train_test_split(df[['height','weight']], df['label'], test_size=0.3, random_state=0)\n",
    "\n",
    "# Normalization\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x_data_train)\n",
    "x_data_train_norm = scaler.transform(x_data_train)\n",
    "x_data_test_norm = scaler.transform(x_data_test)\n",
    "\n",
    "# Logistic Regression\n",
    "model = LogisticRegression()\n",
    "model.fit(x_data_train_norm, t_data_train)\n",
    "print(model.score(x_data_test_norm, t_data_test))\n",
    "\n",
    "\n",
    "# KNN 을 이용한 분류\n",
    "knn_model = KNeighborsClassifier(n_neighbors=3)\n",
    "knn_model.fit(x_data_train_norm, t_data_train)\n",
    "print(knn_model.score(x_data_test_norm, t_data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn: [[29.58893827]]\n",
      "tensorflow:[[29.775517]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')   # warning 출력을 하지 않음\n",
    "\n",
    "# Raw Data Loading\n",
    "df = pd.read_csv('./data/ozone.csv')\n",
    "\n",
    "training_data = df\n",
    "\n",
    "x_data = training_data[['Solar.R', 'Wind', 'Temp']]\n",
    "t_data = training_data['Ozone']\n",
    "\n",
    "# 결측치 확인\n",
    "# 1. 독립변수에 대한 결측치 처리부터 하고 가자\n",
    "# Solar.R의 7개의 결측치를 median으로 처리할 거다\n",
    "for col in x_data.columns:\n",
    "    col_median = np.nanmedian(x_data[col])\n",
    "    x_data[col].loc[x_data[col].isnull()] = col_median\n",
    "\n",
    "# 2. 독립변수에 대한 이상치를 검출 한 후 이상치는 mean처리할 거다\n",
    "zscore = 1.8\n",
    "\n",
    "for col in x_data.columns:\n",
    "    outliers = x_data[col][np.abs(stats.zscore(x_data[col])) > zscore ]\n",
    "#     print(outliers)\n",
    "    col_mean = np.mean(x_data.loc[~x_data[col].isin(outliers),col])\n",
    "    x_data.loc[~x_data[col].isin(outliers),col] = col_mean\n",
    "\n",
    "# 3. 정규화를 진행하자\n",
    "\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_t = MinMaxScaler()\n",
    "\n",
    "scaler_x.fit(x_data.values)\n",
    "scaler_t.fit(t_data.values.reshape(-1,1))\n",
    "\n",
    "x_data_norm = scaler_x.transform(x_data)\n",
    "t_data_norm = scaler_t.transform(t_data.values.reshape(-1,1)).ravel()\n",
    "\n",
    "# 4. 종속변수(Ozone)에 대한 결측치는 KNN을 이용해서 예측값으로 imputation할거다\n",
    "#   학습에 사용될 x_data_train_norm, t_data_train_norm을 구해야한다.\n",
    "x_data_train_norm = x_data_norm[~np.isnan(t_data_norm)]\n",
    "t_data_train_norm = t_data_norm[~np.isnan(t_data_norm)]\n",
    "\n",
    "knn_regressor = KNeighborsRegressor(n_neighbors=2)\n",
    "knn_regressor.fit(x_data_train_norm, t_data_train_norm)\n",
    "\n",
    "# knn_predict\n",
    "knn_predict = knn_regressor.predict(x_data_norm[np.isnan(t_data_norm)])\n",
    "t_data_norm[np.isnan(t_data_norm)] = knn_predict\n",
    "\n",
    "# 최종 데이터를 생성했다.\n",
    "\n",
    "##########################\n",
    "# 학습을 진행해보자 Linear Regression\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.optimizers import SGD      # stokastic Gradient Descent\n",
    "\n",
    "test_data = [[310, 15, 80]]\n",
    "\n",
    "# sklearn\n",
    "sklearn_model = LinearRegression()\n",
    "sklearn_model.fit(x_data_norm, t_data_norm)\n",
    "result = sklearn_model.predict(scaler_x.transform(test_data)).reshape(-1,1)\n",
    "\n",
    "scaled_result = scaler_t.inverse_transform(result)\n",
    "print('sklearn: {}'.format(scaled_result))\n",
    "\n",
    "# Tensorflow 2.x\n",
    "# 모델생성\n",
    "keras_model = Sequential()\n",
    "# 레이어 추가\n",
    "keras_model.add(Flatten(input_shape=(3,)))   # input layer\n",
    "keras_model.add(Dense(1, activation='linear'))  # output layer\n",
    "\n",
    "# complie\n",
    "keras_model.compile(optimizer=SGD(learning_rate=1e-2),\n",
    "                    loss='mse')\n",
    "\n",
    "# 학습\n",
    "keras_model.fit(x_data_norm, t_data_norm, epochs=5000, verbose=0) \n",
    "#verbose는 epoch마다 loss찍어줌\n",
    "\n",
    "# prediction\n",
    "result = keras_model.predict(scaler_x.transform(test_data)).reshape(-1,1)\n",
    "\n",
    "scaled_result = scaler_t.inverse_transform(result)\n",
    "print('tensorflow:{}'.format(scaled_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LogisticRegression에 대해서 sklearn과 Tensorflow 2.x 구현\n",
    "\n",
    "## titanic => logistic 문제 (결측치 다수)\n",
    "## feature engineering\n",
    "\n",
    "## 데이터를 완전히 준비하고 구현해보자\n",
    "\n",
    "%reset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.optimizers import SGD \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy import stats\n",
    "\n",
    "# Raw Data Loading\n",
    "df = pd.read_csv('./data/titanic/train.csv')\n",
    "\n",
    "df = df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin', 'Fare'], axis=1, inplace=False)\n",
    "\n",
    "df['Family'] = df['SibSp'] + df['Parch']\n",
    "df.drop(['SibSp', 'Parch'], axis=1, inplace=True)\n",
    "\n",
    "sex_dict = {'male': 0 ,'female' : 1}\n",
    "df['Sex'] = df['Sex'].map(sex_dict)\n",
    "embarked_dict = {'S' : 0 , 'C' : 1, 'Q' :2}\n",
    "df['Embarked'] = df['Embarked'].map(embarked_dict)\n",
    "\n",
    "## 결측치 처리\n",
    "df.loc[df['Age'].isnull(),'Age'] = np.nanmedian(df['Age'].values)\n",
    "\n",
    "\n",
    "############\n",
    "\n",
    "# sklearn으로 모델 구현\n",
    "sklearn_model = LogisticRegression()\n",
    "sklearn_model.fit(x_data_train_norm, t_data_train)\n",
    "sklearn_result = sklearn_model.score(x_data_test_norm, t_data_test)\n",
    "\n",
    "print('sklearn 정확도 : {}'.format(sklearn_result))\n",
    "\n",
    "# tensorflow 2.x로 구현\n",
    "keras_model = Sequential()\n",
    "\n",
    "keras_model.add(Flatten(input_shape=(x_data_train_norm.shape[1],)))\n",
    "keras_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "keras_model.compile(optimizers=SGD(learning_rate=1e-3),\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "result= keras_model.fit(x_data_train_nrom,\n",
    "                        t_data_train,\n",
    "                        epochs=1000,\n",
    "                        verbose=1,\n",
    "                        validation_split=0.3)\n",
    "\n",
    "keras_result = keras_model.evalutate(x_data_test_norm, t_data_test)\n",
    "print('TF : {}'.format(keras_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(result.history.keys())\n",
    "\n",
    "plt.plot(result.history['accuracy'], color='b')\n",
    "plt.plot(result.history['val_accuracy'], color='r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n",
      "sklearn result :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96      1242\n",
      "           1       0.95      0.97      0.96      1429\n",
      "           2       0.92      0.90      0.91      1276\n",
      "           3       0.91      0.90      0.91      1298\n",
      "           4       0.92      0.92      0.92      1236\n",
      "           5       0.88      0.88      0.88      1119\n",
      "           6       0.93      0.96      0.94      1243\n",
      "           7       0.94      0.93      0.94      1334\n",
      "           8       0.89      0.88      0.88      1204\n",
      "           9       0.89      0.89      0.89      1219\n",
      "\n",
      "    accuracy                           0.92     12600\n",
      "   macro avg       0.92      0.92      0.92     12600\n",
      "weighted avg       0.92      0.92      0.92     12600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Multinomial Classification에 대해서 구현해보자\n",
    "# MNIST 예제를 이용해서 구현해보자\n",
    "\n",
    "%reset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.optimizers import SGD \n",
    "\n",
    "# Raw Data Loading\n",
    "df = pd.read_csv('./data/digit-recognizer/train.csv')\n",
    "\n",
    "# 결측치 이상치는 없다. Feature Engineering 필요없다.\n",
    "\n",
    "# 독립변수와 종속변수 분리\n",
    "x_data = df.drop('label', axis=1, inplace=False)\n",
    "t_data = df['label']   # one-hot encoding 일단 안함\n",
    "\n",
    "# Normalization\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x_data)\n",
    "x_data_norm = scaler.transform(x_data)\n",
    "\n",
    "# Data Split\n",
    "x_data_train, x_data_test, t_data_train, t_data_test = \\\n",
    "train_test_split(x_data_norm, t_data, test_size=0.3, random_state=0)\n",
    "\n",
    "# 데이터 준비가 끝났다. 학습 진행\n",
    "sklearn_model = LogisticRegression(solver='saga')\n",
    "# solver라는 개념이 있는데 default로 사용되는건 lbfgs라는 놈\n",
    "# lbfgs: 작은 데이터셋에 좋다. 데이터량이 많으면 성능이 별로다.\n",
    "# 데이터량이 많은 경우 sag(Stochastic Average Gradient Descent)를 사용하면 더 좋다\n",
    "# 일반적으로 또 이걸 개량한 saga를 더 많이 이용한다.\n",
    "\n",
    "sklearn_model.fit(x_data_train, t_data_train)  # 학습진행\n",
    "print('sklearn result :')\n",
    "print(classification_report(t_data_test,\n",
    "                            sklearn_model.predict(x_data_test)))\n",
    "\n",
    "# TF 2.0 구현\n",
    "keras_model = Sequential()\n",
    "keras_model.add(Flatten(input_shape=(x_data_train.shape[1],)))\n",
    "keras_model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "keras_model.compule(optimizer=SGD(learning_rate=1e-2),\n",
    "                    loss='sparse_categorical_crossentropy',\n",
    "                    metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "history = keras_model.fit(x_data_train,\n",
    "                          t_data_train,\n",
    "                          epochs=500,\n",
    "                          batch_size=100,\n",
    "                          verbose=0,\n",
    "                          validation_split=0.3)\n",
    "\n",
    "print(keras_model.evaluate(x_data_test, t_data_test))\n",
    "print('tensorflow result :')\n",
    "print(classification_report(t_data_test,\n",
    "                            keras_model.predict(x_data_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(classification_report(t_data_test,\n",
    "#                             (tf.argmax(keras_model.predict(x_data_test), axis=1)).numpy())\n",
    "\n",
    "# (tf.argmax(keras_model.predict(x_data_test), axis=1)).numpy()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(history.history.keys())\n",
    "\n",
    "plt.plot(history.history['accuracy'], color='b')\n",
    "plt.plot(history.history['val_accuracy'], color='r')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data_env_tensorflow2]",
   "language": "python",
   "name": "conda-env-data_env_tensorflow2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
