{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXzubQT8uaBz"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J70j8939ui1s"
      },
      "source": [
        "!pip uninstall tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFQY0Bqmwg7b"
      },
      "source": [
        "!pip install tensorflow==1.15"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaAHiEC4w5sW"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "df = pd.read_csv('/content/drive/My Drive/MachineLearning/train.csv')\n",
        "display(df.head())\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OVT-6uq4gP4"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_true = [0, 1, 2, 2, 2]   # 정답\n",
        "y_pred = [0, 0, 2, 2, 1]   # 우리 model이 예측한 값\n",
        "\n",
        "target_name = ['thin', 'normal', 'fat']\n",
        "\n",
        "print(classification_report(y_true, y_pred, target_names = target_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3O4BZ3ICTPr"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "y_true = [2, 0, 2, 2, 0, 1]\n",
        "y_pred = [0, 0, 2, 2, 0, 2]\n",
        "\n",
        "confusion_matrix(y_true,y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8yr-_AmaxoS"
      },
      "source": [
        "# Tensorflow 1.15버전을 가지고 MNIST예제를 구현\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns           # confusino matrix를 heatmap을 통해서 그래프 출력\n",
        "from sklearn.preprocessing import MinMaxScaler   # Normalization\n",
        "from sklearn.model_selection import train_test_split   # train, test 분리\n",
        "from sklearn.model_selection import KFold         # Cross Validation\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# 1. Raw Data Loading\n",
        "df = pd.read_csv('/content/drive/My Drive/MachineLearning/train.csv')\n",
        "# display(df.head(), df.shape)      # (42000, 785)\n",
        "\n",
        "# 2. 결측치와 이상치 처리 => 결측치를 찾고 만약 결측치가 있으면 수정하자 이상치는 (scipy zscore이용)\n",
        "# 근데 MSNIST에는 없네\n",
        "\n",
        "# 3. 사용하는 데이터가 이미지 데이터다 => 어떤 이미지인지 한번 확인해보자\n",
        "#    df에서 label column은 제외하고 pixel 데이터만 들고오자\n",
        "img_data = df.drop('label', axis=1, inplace=False).values\n",
        "# 이미지들의 pixel 데이터만 ndarray로 추출(2차원) => 화면에 출력\n",
        "fig = plt.figure()  # 출력할 전체 화면을 지칭하는 객체를 가져온다.\n",
        "# fig안에 subplot을 만들거다. 저장할 list 만든다.\n",
        "fig_arr = list()\n",
        "\n",
        "for n in range(10):\n",
        "  fig_arr.append(fig.add_subplot(2,5,n+1))\n",
        "  fig_arr[n].imshow(img_data[n].reshape(28,28), cmap='Greys', interpolation='nearest')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 4. Data Split\n",
        "#    데이터는 크게 3부분으로 나누어야 한다.\n",
        "#    일단 2부분으로 나누자 (train용, test용)\n",
        "#    여기서 train용이라고 되어 있는 데이터를 다시 2부분으로 분리 (train, validation)\n",
        "#    train : 학습용 ,  validation : 모델 수정용도의 데이터 셋\n",
        "x_data_train, x_data_test, t_data_train, t_data_test = \\\n",
        "train_test_split(df.drop('label',axis=1), df['label'], test_size=0.3, random_state=0)\n",
        "\n",
        "#5. norm\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(x_data_train)\n",
        "x_data_train_norm = scaler.transform(x_data_train)\n",
        "x_data_test_norm = scaler.transform(x_data_test)\n",
        "\n",
        "# 6 one hot\n",
        "sess = tf.Session()\n",
        "t_data_train_onehot = sess.run(tf.one_hot(t_data_train, depth=10))\n",
        "t_data_test_onehot = sess.run(tf.one_hot(t_data_test, depth=10))\n",
        "\n",
        "#########################\n",
        "# Tensorflow 구현\n",
        "\n",
        "# 1. placeholder\n",
        "X = tf.placeholder(shape = [None,784], dtype=tf.float32)\n",
        "T = tf.placeholder(shape = [None,10], dtype=tf.float32)\n",
        "\n",
        "# 2. Weight,bias\n",
        "W = tf.Variable(tf.random.normal([784,10]), name='weight')\n",
        "b = tf.Variable(tf.random.normal([10]), name='bias')\n",
        "\n",
        "# 3. Model(Hypothesis)\n",
        "logit = tf.matmul(X,W) + b\n",
        "H = tf.nn.softmax(logit)\n",
        "\n",
        "# 4. loss function\n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logit, labels = T))\n",
        "\n",
        "# 5. Optimizer를 이용한 train (Optimizer는 loss값을 줄이는 알고리즘)\n",
        "train = tf.train.GradientDescentOptimizer(learning_rate = 1e-1).minimize(loss)\n",
        "\n",
        "# parameter setting (기본적으로 2개는 설정)\n",
        "num_of_epoch = 100\n",
        "batch_size = 100\n",
        "\n",
        "# 7. 학습진행\n",
        "def run_train(sess ,train_x, train_t,):\n",
        "  print('###학습시작###')\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  total_batch = int(train_x.shape[0] / batch_size)\n",
        "  for step in range(num_of_epoch):\n",
        "    \n",
        "    for i in range(total_batch):\n",
        "      batch_x = train_x[i*batch_size:(i+1)*batch_size]\n",
        "      batch_t = train_t[i*batch_size:(i+1)*batch_size]\n",
        "      _, loss_val = sess.run([train,loss], feed_dict={X:batch_x, T:batch_t})\n",
        "\n",
        "    if step % 10 == 0:\n",
        "      print('Loss: {}'.format(loss_val))\n",
        "  print('###학습끝###')\n",
        "\n",
        "# Accuracy\n",
        "predict = tf.argmax(H, 1)     # [[0.1 0.3 0.2 .... 0.1]]\n",
        "\n",
        "\n",
        "# sklearn을 이용해서 classification_report를 출력\n",
        "target_name = ['num 0', 'num 1', 'num 2', 'num 3', 'num 4', 'num 5', 'num 6', 'num 7', 'num 8', 'num 9']\n",
        "# train 데이터로 학습하고 train 데이터로 성능평가를 해보자\n",
        "run_train(sess,x_data_train_norm,t_data_train_onehot)\n",
        "print(classification_report(t_data_train, sess.run(predict, feed_dict={X:x_data_train_norm}), target_names= target_name))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgOPz6Yk9x7D"
      },
      "source": [
        "# seaborn을 이용한 confusion matrix의 그래프 출력\n",
        "fig, ax = plt.subplots(figsize=(10, 10))   # inch단위로 그림의 크기\n",
        "sns.heatmap(\n",
        "    confusion_matrix(t_data_train, sess.run(predict, feed_dict={X:x_data_train_norm})),\n",
        "    annot = True, # 숫자표현\n",
        "    cbar = True,  # color bar\n",
        "    fmt = '3d',   # 정수표현\n",
        "    cmap = 'Blues',  # color 색상\n",
        "    ax = ax        # 그래프로 사용할 subplot\n",
        ")\n",
        "ax.set_xlabel('Predict')\n",
        "ax.set_ylabel('Actual')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}